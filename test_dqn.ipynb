{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorboardX import SummaryWriter\n",
    "sess = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import collections\n",
    "gpu_options = tf.GPUOptions(allow_growth=True,per_process_gpu_memory_fraction=0.8)\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trrrrr/anaconda3/envs/rl/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
      "(84, 84, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from wrappers import make_atari_deepmind\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "env_name = \"PongNoFrameskip-v4\"\n",
    "#env_name = \"BreakoutNoFrameskip-v4\"\n",
    "#env_name = \"SpaceInvadersNoFrameskip-v4\"\n",
    "env = make_atari_deepmind(env_name, skip=4)\n",
    "observation_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "print(n_actions)\n",
    "print(env.unwrapped.get_action_meanings())\n",
    "\n",
    "obs = env.reset()\n",
    "obs, r, done, _ = env.step(2)\n",
    "\n",
    "done = False\n",
    "for _ in range(130):\n",
    "    obs, _, done, _ = env.step(1)\n",
    "    #env.render()\n",
    "    obs = np.array(obs)\n",
    "    if done:\n",
    "        print('done')\n",
    "        break\n",
    "plt.title(\"what your network gonna see\")\n",
    "print (obs.shape)\n",
    "plt.imshow(obs[:,:,0],interpolation='none',cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0725 16:35:38.650569 139620787283776 deprecation_wrapper.py:119] From /home/trrrrr/Documents/github/ml/dqn_atari/dqnagent.py:80: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0725 16:35:38.656374 139620787283776 deprecation_wrapper.py:119] From /home/trrrrr/Documents/github/ml/dqn_atari/networks.py:297: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0725 16:35:38.656860 139620787283776 deprecation.py:323] From /home/trrrrr/Documents/github/ml/dqn_atari/networks.py:181: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "W0725 16:35:38.659631 139620787283776 deprecation.py:506] From /home/trrrrr/anaconda3/envs/rl/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0725 16:35:39.025816 139620787283776 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0725 16:35:39.026374 139620787283776 deprecation.py:323] From /home/trrrrr/anaconda3/envs/rl/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "W0725 16:35:39.248499 139620787283776 deprecation_wrapper.py:119] From /home/trrrrr/Documents/github/ml/dqn_atari/networks.py:7: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "W0725 16:35:39.256273 139620787283776 deprecation_wrapper.py:119] From /home/trrrrr/Documents/github/ml/dqn_atari/networks.py:29: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W0725 16:35:39.504275 139620787283776 deprecation_wrapper.py:119] From /home/trrrrr/Documents/github/ml/dqn_atari/dqnagent.py:110: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "W0725 16:35:39.511765 139620787283776 deprecation_wrapper.py:119] From /home/trrrrr/Documents/github/ml/dqn_atari/dqnagent.py:133: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
      "\n",
      "W0725 16:35:39.512124 139620787283776 deprecation_wrapper.py:119] From /home/trrrrr/Documents/github/ml/dqn_atari/dqnagent.py:133: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
      "\n",
      "W0725 16:35:39.521700 139620787283776 deprecation_wrapper.py:119] From /home/trrrrr/Documents/github/ml/dqn_atari/dqnagent.py:136: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W0725 16:35:39.561298 139620787283776 deprecation.py:323] From /home/trrrrr/anaconda3/envs/rl/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0725 16:35:39.869450 139620787283776 deprecation_wrapper.py:119] From /home/trrrrr/Documents/github/ml/dqn_atari/dqnagent.py:94: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dqnagent import DQNAgent\n",
    "import tr_helpers\n",
    "import models\n",
    "\n",
    "\n",
    "#agent.epsilon = 0.5\n",
    "\n",
    "breakout_dddqn_config = {\n",
    "    'GAMMA' : 0.99,\n",
    "    'LEARNING_RATE' : 1e-4,\n",
    "    'STEPS_PER_EPOCH' : 8,\n",
    "    'BATCH_SIZE' : 32 * 2,\n",
    "    'EPSILON' : 0,\n",
    "    'MIN_EPSILON' : 0,\n",
    "    'EPSILON_DECAY_FRAMES' : 100000,\n",
    "    'NUM_EPOCHS_TO_COPY' : 1000,\n",
    "    'NAME' : 'ENDDDQN2',\n",
    "    'IS_DOUBLE' : False,\n",
    "    'DUELING_TYPE' : 'AVERAGE',\n",
    "    'SCORE_TO_WIN' : 420,\n",
    "    'NUM_STEPS_FILL_BUFFER' : 100000,\n",
    "    'REPLAY_BUFFER_TYPE' : 'normal',\n",
    "    'REPLAY_BUFFER_SIZE' : 100000,\n",
    "    'PRIORITY_BETA' : 0.4,\n",
    "    'PRIORITY_ALPHA' : 0.6,\n",
    "    'BETA_DECAY_FRAMES' : 1e5,\n",
    "    'MAX_BETA' : 1,\n",
    "    'STEPS_NUM' : 3,\n",
    "    'NETWORK' : models.AtariNoisyDuelingDQN(),\n",
    "    'REWARD_SHAPER' : tr_helpers.DefaultRewardsShaper(),\n",
    "    'EPISODES_TO_LOG' : 20, \n",
    "    'LIVES_REWARD' : 5,\n",
    "    'ATOMS_NUM' : 1\n",
    "    }\n",
    "\n",
    "spaceinviders_dddqn_config = {\n",
    "    'GAMMA' : 0.99,\n",
    "    'LEARNING_RATE' : 1e-4,\n",
    "    'STEPS_PER_EPOCH' : 8,\n",
    "    'BATCH_SIZE' : 32 * 2,\n",
    "    'EPSILON' : 0,\n",
    "    'MIN_EPSILON' : 0,\n",
    "    'EPSILON_DECAY_FRAMES' : 100000,\n",
    "    'NUM_EPOCHS_TO_COPY' : 1000,\n",
    "    'NAME' : 'ENDDDQN2',\n",
    "    'IS_DOUBLE' : True,\n",
    "    'DUELING_TYPE' : 'AVERAGE',\n",
    "    'SCORE_TO_WIN' : 5000,\n",
    "    'NUM_STEPS_FILL_BUFFER' : 10000,\n",
    "    'REPLAY_BUFFER_TYPE' : 'normal',\n",
    "    'REPLAY_BUFFER_SIZE' : 100000,\n",
    "    'PRIORITY_BETA' : 0.4,\n",
    "    'PRIORITY_ALPHA' : 0.6,\n",
    "    'BETA_DECAY_FRAMES' : 1e5,\n",
    "    'MAX_BETA' : 1,\n",
    "    'STEPS_NUM' : 1,\n",
    "    'NETWORK' : models.AtariNoisyDuelingDQN(),\n",
    "    'REWARD_SHAPER' : tr_helpers.DefaultRewardsShaper(),#tr_helpers.DefaultRewardsShaper(clip_value=3, scale_value = 0.05),\n",
    "    'EPISODES_TO_LOG' : 15, \n",
    "    'LIVES_REWARD' : 3,\n",
    "    'ATOMS_NUM' : 1\n",
    "    }\n",
    "\n",
    "pong_dddqn_config = {\n",
    "    'GAMMA' : 0.99,\n",
    "    'LEARNING_RATE' : 1e-4,\n",
    "    'STEPS_PER_EPOCH' : 4,\n",
    "    'BATCH_SIZE' : 32 * 4,\n",
    "    'EPSILON' : .90,\n",
    "    'MIN_EPSILON' : 0.02,\n",
    "    'EPSILON_DECAY_FRAMES' : 100000,\n",
    "    'NUM_EPOCHS_TO_COPY' : 1000,\n",
    "    'NAME' : 'PongDDDQN',\n",
    "    'IS_DOUBLE' : True,\n",
    "    'DUELING_TYPE' : 'AVERAGE',\n",
    "    'SCORE_TO_WIN' : 20,\n",
    "    'NUM_STEPS_FILL_BUFFER' : 10000,\n",
    "    'REPLAY_BUFFER_TYPE' : 'normal',\n",
    "    'REPLAY_BUFFER_SIZE' : 100000,\n",
    "    'PRIORITY_BETA' : 0.4,\n",
    "    'PRIORITY_ALPHA' : 0.6,\n",
    "    'BETA_DECAY_FRAMES' : 1e5,\n",
    "    'MAX_BETA' : 1,\n",
    "    'NETWORK' : models.AtariDuelingDQN(),\n",
    "    'STEPS_NUM' : 1,\n",
    "    'REWARD_SHAPER' : tr_helpers.DefaultRewardsShaper(),\n",
    "    'EPISODES_TO_LOG' : 10, \n",
    "    'LIVES_REWARD' : 1,\n",
    "    'ATOMS_NUM' : 1\n",
    "    }\n",
    "\n",
    "pong_dddqn_config7 = {\n",
    "    'GAMMA' : 0.99,\n",
    "    'LEARNING_RATE' : 1e-4,\n",
    "    'STEPS_PER_EPOCH' : 4,\n",
    "    'BATCH_SIZE' : 32 * 4,\n",
    "    'EPSILON' : 0,\n",
    "    'MIN_EPSILON' : 0,\n",
    "    'EPSILON_DECAY_FRAMES' : 100000,\n",
    "    'NUM_EPOCHS_TO_COPY' : 1000,\n",
    "    'NAME' : 'pong_dddqn_config6',\n",
    "    'IS_DOUBLE' : True,\n",
    "    'DUELING_TYPE' : 'AVERAGE',\n",
    "    'SCORE_TO_WIN' : 18,\n",
    "    'NUM_STEPS_FILL_BUFFER' : 1000,\n",
    "    'REPLAY_BUFFER_TYPE' : 'normal',\n",
    "    'REPLAY_BUFFER_SIZE' : 1000,\n",
    "    'PRIORITY_BETA' : 0.4,\n",
    "    'PRIORITY_ALPHA' : 0.6,\n",
    "    'BETA_DECAY_FRAMES' : 1e5,\n",
    "    'MAX_BETA' : 1,\n",
    "    'NETWORK' : models.AtariNoisyDuelingDQN(),\n",
    "    'STEPS_NUM' : 3,\n",
    "    'REWARD_SHAPER' : tr_helpers.DefaultRewardsShaper(),\n",
    "    'EPISODES_TO_LOG' : 10, \n",
    "    'LIVES_REWARD' : 1,\n",
    "    'ATOMS_NUM' : 1\n",
    "    }\n",
    "pong_dddqn_config0 = {\n",
    "    'GAMMA' : 0.99,\n",
    "    'LEARNING_RATE' : 1e-4,\n",
    "    'STEPS_PER_EPOCH' : 4,\n",
    "    'BATCH_SIZE' : 32 * 4,\n",
    "    'EPSILON' : .90,\n",
    "    'MIN_EPSILON' : 0.02,\n",
    "    'EPSILON_DECAY_FRAMES' : 100000,\n",
    "    'NUM_EPOCHS_TO_COPY' : 1000,\n",
    "    'NAME' : 'pong_dddqn_config0',\n",
    "    'IS_DOUBLE' : False,\n",
    "    'DUELING_TYPE' : 'AVERAGE',\n",
    "    'SCORE_TO_WIN' : 18,\n",
    "    'NUM_STEPS_FILL_BUFFER' : 10000,\n",
    "    'REPLAY_BUFFER_TYPE' : 'prioritized',\n",
    "    'REPLAY_BUFFER_SIZE' : 100000,\n",
    "    'PRIORITY_BETA' : 0.4,\n",
    "    'PRIORITY_ALPHA' : 0.6,\n",
    "    'BETA_DECAY_FRAMES' : 1e5,\n",
    "    'MAX_BETA' : 1,\n",
    "    'NETWORK' : models.AtariDQN(),\n",
    "    'STEPS_NUM' : 1,\n",
    "    'REWARD_SHAPER' : tr_helpers.DefaultRewardsShaper(),\n",
    "    'EPISODES_TO_LOG' : 10, \n",
    "    'LIVES_REWARD' : 1,\n",
    "    'ATOMS_NUM' : 1\n",
    "    }\n",
    "\n",
    "pong_dddqn_config1 = {\n",
    "    'GAMMA' : 0.99,\n",
    "    'LEARNING_RATE' : 1e-4,\n",
    "    'STEPS_PER_EPOCH' : 4,\n",
    "    'BATCH_SIZE' : 32 * 4,\n",
    "    'EPSILON' : .90,\n",
    "    'MIN_EPSILON' : 0.02,\n",
    "    'EPSILON_DECAY_FRAMES' : 100000,\n",
    "    'NUM_EPOCHS_TO_COPY' : 1000,\n",
    "    'NAME' : 'pong_dddqn_config1',\n",
    "    'IS_DOUBLE' : False,\n",
    "    'DUELING_TYPE' : 'AVERAGE',\n",
    "    'SCORE_TO_WIN' : 18,\n",
    "    'NUM_STEPS_FILL_BUFFER' : 10000,\n",
    "    'REPLAY_BUFFER_TYPE' : 'normal',\n",
    "    'REPLAY_BUFFER_SIZE' : 100000,\n",
    "    'PRIORITY_BETA' : 0.4,\n",
    "    'PRIORITY_ALPHA' : 0.6,\n",
    "    'BETA_DECAY_FRAMES' : 1e5,\n",
    "    'MAX_BETA' : 1,\n",
    "    'NETWORK' : models.AtariDQN(),\n",
    "    'STEPS_NUM' : 1,\n",
    "    'REWARD_SHAPER' : tr_helpers.DefaultRewardsShaper(),\n",
    "    'EPISODES_TO_LOG' : 10, \n",
    "    'LIVES_REWARD' : 1,\n",
    "    'ATOMS_NUM' : 1\n",
    "    }\n",
    "pong_dddqn_config3 = {\n",
    "    'GAMMA' : 0.99,\n",
    "    'LEARNING_RATE' : 1e-4,\n",
    "    'STEPS_PER_EPOCH' : 4,\n",
    "    'BATCH_SIZE' : 32 * 4,\n",
    "    'EPSILON' : .90,\n",
    "    'MIN_EPSILON' : 0.02,\n",
    "    'EPSILON_DECAY_FRAMES' : 100000,\n",
    "    'NUM_EPOCHS_TO_COPY' : 1000,\n",
    "    'NAME' : 'pong_dddqn_config3',\n",
    "    'IS_DOUBLE' : True,\n",
    "    'DUELING_TYPE' : 'AVERAGE',\n",
    "    'SCORE_TO_WIN' : 18,\n",
    "    'NUM_STEPS_FILL_BUFFER' : 10000,\n",
    "    'REPLAY_BUFFER_TYPE' : 'normal',\n",
    "    'REPLAY_BUFFER_SIZE' : 100000,\n",
    "    'PRIORITY_BETA' : 0.4,\n",
    "    'PRIORITY_ALPHA' : 0.6,\n",
    "    'BETA_DECAY_FRAMES' : 1e5,\n",
    "    'MAX_BETA' : 1,\n",
    "    'NETWORK' : models.AtariDuelingDQN(),\n",
    "    'STEPS_NUM' : 1,\n",
    "    'REWARD_SHAPER' : tr_helpers.DefaultRewardsShaper(),\n",
    "    'EPISODES_TO_LOG' : 10, \n",
    "    'LIVES_REWARD' : 1,\n",
    "    'ATOMS_NUM' : 1\n",
    "    }\n",
    "agent = DQNAgent(env, sess, env_name, config = pong_dddqn_config7)\n",
    "#agent.restore('nn/pong_dddqn_config0PongNoFrameskip-v4')\n",
    "#agent.epsilon = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per seconds:  161.89008452257698\n",
      "Frames per seconds:  168.86622400135792\n",
      "Frames per seconds:  168.38457469817726\n",
      "Frames per seconds:  169.266136776416\n",
      "Frames per seconds:  169.16402126730483\n",
      "Frames per seconds:  170.0712856826292\n",
      "Frames per seconds:  170.3858932330667\n",
      "saving next best rewards:  -19.9\n",
      "Frames per seconds:  168.58282083891166\n",
      "Frames per seconds:  167.36521482745135\n",
      "Frames per seconds:  168.73586451773744\n",
      "Frames per seconds:  168.46571312505026\n",
      "Frames per seconds:  168.0724516879819\n",
      "Frames per seconds:  169.52577530595082\n",
      "Frames per seconds:  170.32312348506045\n",
      "Frames per seconds:  168.35872854459853\n",
      "saving next best rewards:  -19.8\n",
      "Frames per seconds:  168.22108825380116\n",
      "Frames per seconds:  168.6844595361988\n",
      "Frames per seconds:  168.19639841178608\n",
      "Frames per seconds:  168.66124086509626\n",
      "Frames per seconds:  168.727400051749\n",
      "Frames per seconds:  169.01272693673377\n",
      "Frames per seconds:  169.02235753060518\n",
      "Frames per seconds:  166.5496985367164\n",
      "Frames per seconds:  167.60694648071504\n",
      "Frames per seconds:  168.19029452741873\n",
      "saving next best rewards:  -19.7\n",
      "Frames per seconds:  168.03888476100346\n",
      "Frames per seconds:  167.72348664417586\n",
      "Frames per seconds:  169.19652401779138\n",
      "Frames per seconds:  168.8056829220311\n",
      "Frames per seconds:  168.76842023686868\n",
      "Frames per seconds:  168.04633094987133\n",
      "Frames per seconds:  168.67562035776024\n",
      "Frames per seconds:  169.0004008331269\n",
      "Frames per seconds:  168.7087499997989\n",
      "Frames per seconds:  168.04934731543597\n",
      "Frames per seconds:  168.93961382252832\n",
      "Frames per seconds:  168.33364697951902\n",
      "Frames per seconds:  166.80481882249185\n",
      "Frames per seconds:  164.5835382793995\n",
      "Frames per seconds:  165.0886505882343\n",
      "Frames per seconds:  166.1525646150557\n",
      "Frames per seconds:  167.26800086395596\n",
      "Frames per seconds:  166.61126601401944\n",
      "Frames per seconds:  166.81256073561474\n",
      "Frames per seconds:  166.2123369991844\n",
      "Frames per seconds:  166.65992499178085\n",
      "Frames per seconds:  166.50393284431144\n",
      "Frames per seconds:  166.24351760690104\n",
      "Frames per seconds:  166.06741816823256\n",
      "Frames per seconds:  165.7348150653645\n",
      "Frames per seconds:  165.9914173539403\n",
      "Frames per seconds:  166.30266969790856\n",
      "Frames per seconds:  165.64095064695795\n",
      "Frames per seconds:  165.89964408126895\n",
      "Frames per seconds:  166.0129867842421\n",
      "Frames per seconds:  165.9463255958521\n",
      "Frames per seconds:  166.11495731820384\n",
      "Frames per seconds:  165.5617320701923\n",
      "Frames per seconds:  166.07979361666273\n",
      "Frames per seconds:  166.49240613073175\n",
      "Frames per seconds:  166.14021780536757\n",
      "Frames per seconds:  166.5980039240241\n",
      "Frames per seconds:  165.87611636936234\n",
      "Frames per seconds:  164.96683950613593\n",
      "Frames per seconds:  161.45380732951008\n",
      "Frames per seconds:  162.94994322223795\n",
      "Frames per seconds:  166.16907377122538\n",
      "Frames per seconds:  164.55202819606887\n",
      "Frames per seconds:  165.3007029066459\n",
      "Frames per seconds:  166.14033626283984\n",
      "Frames per seconds:  165.27983919287496\n",
      "Frames per seconds:  164.14805331740345\n",
      "Frames per seconds:  162.82902890626926\n",
      "Frames per seconds:  165.67333072319977\n",
      "Frames per seconds:  163.55402961252008\n",
      "Frames per seconds:  164.01362379706643\n",
      "Frames per seconds:  166.03931394551162\n",
      "Frames per seconds:  165.6600801216901\n",
      "Frames per seconds:  166.29021486318229\n",
      "Frames per seconds:  165.329196277296\n",
      "Frames per seconds:  165.05860966184798\n",
      "Frames per seconds:  164.46182631070673\n",
      "Frames per seconds:  167.34768594156296\n",
      "Frames per seconds:  166.6822448031019\n",
      "Frames per seconds:  165.86423695148602\n",
      "Frames per seconds:  168.47822528565615\n",
      "Frames per seconds:  167.14523166507064\n",
      "Frames per seconds:  166.476268769593\n",
      "Frames per seconds:  169.75603082989667\n",
      "Frames per seconds:  166.06664229826544\n",
      "Frames per seconds:  166.84461074480973\n",
      "Frames per seconds:  168.28126467564854\n",
      "Frames per seconds:  165.8790553283222\n",
      "Frames per seconds:  166.32573815675266\n",
      "Frames per seconds:  166.24802470915046\n",
      "Frames per seconds:  166.84715934857152\n",
      "saving next best rewards:  -19.6\n",
      "Frames per seconds:  165.76884352598842\n",
      "Frames per seconds:  166.6564285421957\n",
      "Frames per seconds:  166.92447744167657\n",
      "Frames per seconds:  167.2638318339501\n",
      "Frames per seconds:  167.34493507455207\n",
      "Frames per seconds:  166.60973057417323\n",
      "Frames per seconds:  167.21743955471877\n",
      "Frames per seconds:  166.6587594922847\n",
      "Frames per seconds:  166.56805287123805\n",
      "Frames per seconds:  166.52178786018695\n",
      "Frames per seconds:  166.86378691678243\n",
      "Frames per seconds:  167.09598271251622\n",
      "Frames per seconds:  166.41970037896027\n",
      "Frames per seconds:  167.36696457875485\n",
      "Frames per seconds:  166.3292339360963\n",
      "Frames per seconds:  166.35017211275266\n",
      "Frames per seconds:  166.44745794539912\n",
      "Frames per seconds:  165.60097881483696\n",
      "Frames per seconds:  166.30251144585074\n",
      "Frames per seconds:  166.28449905489535\n",
      "Frames per seconds:  166.9935758317849\n",
      "Frames per seconds:  166.7502477814959\n",
      "saving next best rewards:  -19.0\n",
      "Frames per seconds:  166.6705013203726\n",
      "Frames per seconds:  166.62326592174702\n",
      "Frames per seconds:  166.03929422659007\n",
      "Frames per seconds:  166.10908251027047\n",
      "Frames per seconds:  165.37776123681928\n",
      "Frames per seconds:  167.06301093119924\n",
      "Frames per seconds:  165.8884370771516\n",
      "Frames per seconds:  168.1702054226189\n",
      "Frames per seconds:  167.17083313953046\n",
      "Frames per seconds:  167.82663652621642\n",
      "Frames per seconds:  167.65233552275518\n",
      "Frames per seconds:  165.80177829876857\n",
      "Frames per seconds:  166.24645642070288\n",
      "Frames per seconds:  166.7200299040825\n",
      "Frames per seconds:  166.2159795134037\n",
      "Frames per seconds:  165.73477577204324\n",
      "Frames per seconds:  165.82825486516856\n",
      "Frames per seconds:  165.21066121581902\n",
      "Frames per seconds:  165.59268864541693\n",
      "Frames per seconds:  165.70387734377053\n",
      "Frames per seconds:  164.97151124112438\n",
      "Frames per seconds:  165.39758002312402\n",
      "saving next best rewards:  -17.7\n",
      "Frames per seconds:  164.57077782463952\n",
      "Frames per seconds:  165.1688153792604\n",
      "Frames per seconds:  165.91750099042878\n",
      "Frames per seconds:  166.14562753555245\n",
      "Frames per seconds:  165.5026290676998\n",
      "Frames per seconds:  165.8696418428482\n",
      "Frames per seconds:  165.826281449903\n",
      "Frames per seconds:  165.97721599534\n",
      "Frames per seconds:  164.2076326663531\n",
      "Frames per seconds:  163.55637025386523\n",
      "Frames per seconds:  166.03619184127047\n",
      "Frames per seconds:  164.8395198825126\n",
      "Frames per seconds:  165.62132201409045\n",
      "Frames per seconds:  167.66561855934265\n",
      "Frames per seconds:  167.50154450469822\n",
      "Frames per seconds:  168.16992222684783\n",
      "Frames per seconds:  168.10729197298568\n",
      "Frames per seconds:  167.8587616923323\n",
      "Frames per seconds:  161.39926483615045\n",
      "Frames per seconds:  165.27507834038335\n",
      "Frames per seconds:  163.55069415491752\n",
      "Frames per seconds:  164.9229381270815\n",
      "Frames per seconds:  163.90444133601747\n",
      "Frames per seconds:  163.87144923555056\n",
      "Frames per seconds:  167.20680703233455\n",
      "Frames per seconds:  166.3249928492178\n",
      "Frames per seconds:  168.24774252885146\n",
      "Frames per seconds:  169.6657039085289\n",
      "Frames per seconds:  167.40174012551986\n",
      "saving next best rewards:  -15.4\n",
      "Frames per seconds:  166.26134317457266\n",
      "Frames per seconds:  168.97035582843637\n",
      "Frames per seconds:  168.84402917352168\n",
      "Frames per seconds:  167.1015280952647\n",
      "Frames per seconds:  167.76767724766395\n",
      "Frames per seconds:  168.09296205622508\n",
      "Frames per seconds:  168.31755601038935\n",
      "Frames per seconds:  170.3598165836226\n",
      "Frames per seconds:  168.43242859468546\n",
      "Frames per seconds:  163.15039006804327\n",
      "Frames per seconds:  161.42065124009142\n",
      "Frames per seconds:  167.52130020337145\n",
      "Frames per seconds:  168.5531950531093\n",
      "Frames per seconds:  167.56928052068798\n",
      "Frames per seconds:  167.81730956896644\n",
      "Frames per seconds:  167.5786067092508\n",
      "Frames per seconds:  167.8731726454442\n",
      "Frames per seconds:  167.52385614042865\n",
      "Frames per seconds:  168.24155393548\n",
      "Frames per seconds:  167.7468033931991\n",
      "Frames per seconds:  167.4793123735561\n",
      "Frames per seconds:  168.04880193774991\n",
      "Frames per seconds:  170.0607905015485\n",
      "Frames per seconds:  167.75202304715341\n",
      "Frames per seconds:  167.12519164311882\n",
      "Frames per seconds:  166.93127375608302\n",
      "Frames per seconds:  166.5417958440073\n",
      "Frames per seconds:  170.7627137447955\n",
      "Frames per seconds:  164.3778365760658\n",
      "Frames per seconds:  161.74525988466092\n",
      "saving next best rewards:  -11.4\n",
      "Frames per seconds:  159.85532502798884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per seconds:  164.06096961845486\n",
      "Frames per seconds:  162.71183495000173\n",
      "Frames per seconds:  151.81759985011905\n",
      "Frames per seconds:  163.7658319899964\n",
      "Frames per seconds:  165.52083171780689\n",
      "Frames per seconds:  166.97175750435682\n",
      "Frames per seconds:  166.82478871684324\n",
      "Frames per seconds:  170.39257285227177\n",
      "Frames per seconds:  166.56910464753688\n",
      "Frames per seconds:  166.0274834341796\n",
      "Frames per seconds:  166.35250110149977\n",
      "Frames per seconds:  166.12403020639988\n",
      "Frames per seconds:  166.65835554455882\n",
      "Frames per seconds:  166.20399209379661\n",
      "Frames per seconds:  169.98287568943647\n",
      "Frames per seconds:  167.18450645979382\n",
      "Frames per seconds:  166.69799810771343\n",
      "Frames per seconds:  170.52624063891398\n",
      "Frames per seconds:  166.73995298623962\n",
      "Frames per seconds:  169.5586845592958\n",
      "Frames per seconds:  165.98211591201988\n",
      "Frames per seconds:  166.3537678876336\n",
      "Frames per seconds:  166.38483657722563\n",
      "Frames per seconds:  165.383140990462\n",
      "Frames per seconds:  165.06646966121875\n",
      "Frames per seconds:  166.2965376509466\n",
      "Frames per seconds:  166.54277454685007\n",
      "Frames per seconds:  166.08983605064304\n",
      "Frames per seconds:  165.57960781873876\n",
      "Frames per seconds:  166.66993836361374\n",
      "Frames per seconds:  168.80336626021435\n",
      "Frames per seconds:  168.82866275895952\n",
      "Frames per seconds:  169.21246263949485\n",
      "saving next best rewards:  -7.9\n",
      "Frames per seconds:  166.8698679249509\n",
      "Frames per seconds:  164.77473618310373\n",
      "Frames per seconds:  163.8486212536261\n",
      "Frames per seconds:  166.09519645847664\n",
      "Frames per seconds:  156.80852797111461\n",
      "Frames per seconds:  166.73331142980547\n",
      "Frames per seconds:  167.32520759847128\n",
      "Frames per seconds:  169.02940068227477\n",
      "Frames per seconds:  163.50760054472076\n",
      "Frames per seconds:  167.23203397666217\n",
      "Frames per seconds:  165.5203810118237\n",
      "Frames per seconds:  165.66780774573976\n",
      "Frames per seconds:  169.1780430721732\n",
      "Frames per seconds:  169.54667620435708\n",
      "Frames per seconds:  168.46301334066902\n",
      "Frames per seconds:  168.46452223205017\n",
      "Frames per seconds:  168.11969703771638\n",
      "Frames per seconds:  156.90766021247063\n",
      "Frames per seconds:  162.39884348189943\n",
      "Frames per seconds:  162.3368872497604\n",
      "Frames per seconds:  162.98416758936065\n",
      "Frames per seconds:  167.8503648419872\n",
      "Frames per seconds:  165.86955000922813\n",
      "Frames per seconds:  165.99431441421572\n",
      "Frames per seconds:  168.3810798667437\n",
      "Frames per seconds:  168.85597898672418\n",
      "Frames per seconds:  165.7370810117401\n",
      "Frames per seconds:  160.57521729872337\n",
      "Frames per seconds:  162.7050370317211\n",
      "Frames per seconds:  165.0219828444659\n",
      "Frames per seconds:  168.61318231982943\n",
      "Frames per seconds:  168.05751493806423\n",
      "Frames per seconds:  166.94406402035852\n",
      "Frames per seconds:  162.81335368676227\n",
      "Frames per seconds:  167.6302643338467\n",
      "Frames per seconds:  165.6661980372546\n",
      "Frames per seconds:  159.85902325059863\n",
      "Frames per seconds:  161.2214178007756\n",
      "Frames per seconds:  163.8809382265609\n",
      "Frames per seconds:  165.4577306454226\n",
      "Frames per seconds:  164.33292817271305\n",
      "Frames per seconds:  168.64980684884912\n",
      "Frames per seconds:  168.43281413284208\n",
      "Frames per seconds:  158.9798071995955\n",
      "Frames per seconds:  164.34690105313493\n",
      "Frames per seconds:  163.85906141737894\n",
      "Frames per seconds:  163.87406788239338\n",
      "Frames per seconds:  167.46755661063204\n",
      "Frames per seconds:  168.73172381479685\n",
      "Frames per seconds:  167.9260608762719\n",
      "Frames per seconds:  168.19792276585568\n",
      "saving next best rewards:  -5.7\n",
      "Frames per seconds:  167.17743630100534\n",
      "Frames per seconds:  168.4876867686617\n",
      "Frames per seconds:  168.63440118400592\n",
      "Frames per seconds:  168.290217875098\n",
      "Frames per seconds:  166.0908883733519\n",
      "Frames per seconds:  161.58223540948632\n",
      "Frames per seconds:  164.54092505861001\n",
      "Frames per seconds:  166.42411798733784\n",
      "Frames per seconds:  160.91703632252725\n",
      "Frames per seconds:  168.15910755922238\n",
      "Frames per seconds:  168.18673357078265\n",
      "Frames per seconds:  168.98655143430335\n",
      "Frames per seconds:  168.91704595406748\n",
      "Frames per seconds:  163.80182009919798\n",
      "Frames per seconds:  165.74412810800075\n",
      "Frames per seconds:  166.87539830954822\n",
      "Frames per seconds:  169.73145848565838\n",
      "Frames per seconds:  170.70029829281958\n",
      "Frames per seconds:  169.10602776153632\n",
      "Frames per seconds:  169.59271061360334\n",
      "Frames per seconds:  165.06762598874994\n",
      "Frames per seconds:  167.16483011242195\n",
      "Frames per seconds:  168.00352773826003\n",
      "Frames per seconds:  170.17037794316818\n",
      "Frames per seconds:  169.9573147928772\n",
      "Frames per seconds:  166.61640200122224\n",
      "Frames per seconds:  167.87541681137233\n",
      "Frames per seconds:  168.2568204060432\n",
      "Frames per seconds:  170.49278844199586\n",
      "Frames per seconds:  166.2403087517514\n",
      "Frames per seconds:  165.94908319974755\n",
      "Frames per seconds:  166.6558921680229\n",
      "Frames per seconds:  168.02908992081612\n",
      "Frames per seconds:  169.66343221154858\n",
      "Frames per seconds:  165.29215615910624\n",
      "Frames per seconds:  166.88760894406596\n",
      "Frames per seconds:  166.2750988222519\n",
      "Frames per seconds:  168.38944877690432\n",
      "saving next best rewards:  9.2\n",
      "Frames per seconds:  168.77060691048243\n",
      "Frames per seconds:  168.43443747075753\n",
      "Frames per seconds:  167.70729081991087\n",
      "Frames per seconds:  169.78858966549822\n",
      "Frames per seconds:  169.28806692407835\n",
      "Frames per seconds:  168.72133222426743\n",
      "Frames per seconds:  165.0095762743971\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-775f89c97f91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/github/ml/dqn_atari/dqnagent.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTEPS_PER_EPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m                 \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshaped_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                     \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/github/ml/dqn_atari/dqnagent.py\u001b[0m in \u001b[0;36mplay_steps\u001b[0;34m(self, steps, epsilon)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/github/ml/dqn_atari/dqnagent.py\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state, epsilon)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mqvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_qvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/github/ml/dqn_atari/dqnagent.py\u001b[0m in \u001b[0;36mget_qvalues\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_qvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.save(\"./nn/ENDDDQN2\" + env_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trrrrr/anaconda3/envs/rl/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset\n",
      "reward: 16.0\n"
     ]
    }
   ],
   "source": [
    "import wrappers\n",
    "#print(env.unwrapped.get_action_meanings())\n",
    "def evaluate(env,t_max=10000):\n",
    "    rewards = []\n",
    "    env._max_episode_steps = 9999\n",
    "    print('reset')\n",
    "    #env = env.old_env\n",
    "    s = env.reset()\n",
    "    reward = 0\n",
    "    for it in range(t_max):\n",
    "        #nv.render()\n",
    "        #e.render()\n",
    "        qvalues = agent.get_qvalues([s])\n",
    "        action = np.argmax(qvalues)\n",
    "        s, r, done, _ = env.step(action)\n",
    "        reward += r\n",
    "        \n",
    "            \n",
    "        if done:\n",
    "            break       \n",
    "        \n",
    "    return reward\n",
    "\n",
    "import gym.wrappers\n",
    "\n",
    "env_monitor = wrappers.make_atari_deepmind(env_name, noop_max=30, skip=4)\n",
    "#env_monitor = wrappers.ReallyDoneWrapper(env_monitor)\n",
    "env_monitor = gym.wrappers.Monitor(env_monitor,directory='video_dddqn05',force=True)\n",
    "\n",
    "sessions = [print('reward:', evaluate(env_monitor)) for _ in range(1)]\n",
    "env_monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "#agent = DQNAgent(env, sess, ExperienceBuffer(EXP_BUFFER_CAPACITY), env_name, config = dqn_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.reset()\n",
    "#agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
